{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output weights n layer,  validation set, graph plot, test set, lstm cell stack, preloaded full data before training, fetch data from another notebook, added exection time of 1 epoch and remaining time, training with full data set at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "#import csv\n",
    "#from threading import Thread\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from  read_proces_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden units for lstm\n",
    "hidden_units = 32\n",
    "# Lstm layers\n",
    "num_layers = 2\n",
    "# chunks of input given to lstm at a time for training\n",
    "batch_size = 5000\n",
    "# max seq length in training and test data\n",
    "max_seq = 3260\n",
    "# input dimension\n",
    "feature_vec_len = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find max no of times to roll back\n",
    "#max_seq_len = find_max_seq_len()\n",
    "#print max_seq_len\n",
    "max_seq_len = 3259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use ipython feature to read data from another notebook\n",
    "% store -r train_batch\n",
    "% store -r seq_batch\n",
    "% store -r target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create lstm cell in tensorflow\n",
    "def lstm_cell():\n",
    "    return tf.contrib.rnn.BasicLSTMCell(num_units =\\\n",
    "        hidden_units,state_is_tuple=True)\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell(\\\n",
    "        [lstm_cell() for _ in range(num_layers)], \\\n",
    "        state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [batch_size,max_seq,feature_vec_len])\n",
    "s_len = tf.placeholder(tf.int32,shape=[batch_size])\n",
    "# run lstm over different sequence length\n",
    "output_, state = tf.nn.dynamic_rnn(\\\n",
    "        stacked_cell,\\\n",
    "        x,\\\n",
    "#        initial_state = i_state,\\\n",
    "        sequence_length=s_len,\\\n",
    "        dtype=tf.float32)\n",
    "# take all batch, last output of each batch and full output vector\n",
    "def take_subarray(array , index):\n",
    "    return array[range(0,batch_size),index-1]\n",
    "output = tf.py_func(take_subarray,[output_,s_len],tf.float32)\n",
    "#print output\n",
    "# define final output value\n",
    "target_value = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "# define weights and bias from output of lstm cell to network final output\n",
    "output_size = stacked_cell.output_size\n",
    "std = 1/np.sqrt(feature_vec_len,dtype=np.float32)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=std,dtype=tf.float32)\n",
    "    return tf.Variable(initial,dtype=tf.float32)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(std, shape=shape,dtype=tf.float32)\n",
    "    return tf.Variable(initial,dtype=tf.float32)\n",
    "\n",
    "Wo1 = weight_variable([output_size, 2 * output_size])\n",
    "Wo2 = weight_variable([2*output_size, 2 * output_size])\n",
    "Wo3 = weight_variable([2*output_size, 2 * output_size])\n",
    "Wo4 = weight_variable([2*output_size, 1])\n",
    "\n",
    "b1 = bias_variable([2 * output_size])\n",
    "b2 = bias_variable([2 * output_size])\n",
    "b3 = bias_variable([2 * output_size])\n",
    "b4 = bias_variable([1])\n",
    "\n",
    "# initialize variables\n",
    "# calculate final output\n",
    "\n",
    "s1  = tf.matmul(output, Wo1) + b1\n",
    "o1 = tf.nn.relu(s1)\n",
    "\n",
    "s2  = tf.matmul(o1, Wo2) + b2\n",
    "o2 = tf.nn.relu(s2)\n",
    "\n",
    "s3  = tf.matmul(o2, Wo3) + b3\n",
    "o3 = tf.nn.relu(s3)\n",
    "\n",
    "logits  = tf.matmul(o3, Wo4) + b4\n",
    "observed = tf.sigmoid(logits)\n",
    "#observed = tf.nn.relu(logits)\n",
    "\n",
    "# calculate cost as calculated in logistic classification.\n",
    "logistic_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\\\n",
    "        labels = tf.reshape(target_value,[batch_size,1]), logits = logits))\n",
    "\n",
    "# optimize the cost\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(logistic_cost)\n",
    "\n",
    "def threshold_fn(array):\n",
    "    return array >= 0.50\n",
    "obser = tf.py_func(threshold_fn,[observed], tf.bool)\n",
    "observe = tf.cast(obser,tf.float32)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(t,y):\n",
    "    \n",
    "    f1score = metrics.f1_score(t,y)\n",
    "    \n",
    "    fprr, tprr, thresholds = metrics.roc_curve(t, y, pos_label=1)\n",
    "    aucc = metrics.auc(fprr,tprr)\n",
    "    \n",
    "    sk_acc = metrics.accuracy_score(t,y)\n",
    "    \n",
    "    return (f1score, aucc, sk_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_(data, is_train):\n",
    "    # evaluate on all last 5000 values but\n",
    "    # take only last validation set values\n",
    "    y = sess.run(observe, feed_dict = {\\\n",
    "        x:data[0], s_len:data[1]})\n",
    "    if is_train:\n",
    "        c = sess.run(logistic_cost, feed_dict = {target_value:data[2],\\\n",
    "            x:data[0], s_len:data[1]})\n",
    "        f1s,auc,acc = calc_metrics(data[2],y)\n",
    "        train_para.append([f1s, auc, acc, c])\n",
    "    else :\n",
    "        f1s,auc,acc = calc_metrics(data[2][-440:],y[-440:])\n",
    "        vali_para.append([f1s, auc, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_(data):\n",
    "    # train on training data-set\n",
    "    sess.run(train_step, feed_dict = {target_value:data[2],\\\n",
    "            x:data[0], s_len:data[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_parameters(training_p, validation_p):\n",
    "    nu_para = training_p.shape[1]\n",
    "    para_name = ['f1score','auc','accuracy']\n",
    "    for j in range(nu_para):\n",
    "        plt.subplot(nu_para,1,j+1)\n",
    "        plt.plot(training_p[:,j],'r')\n",
    "        plt.plot(validation_p[:,j],'b')\n",
    "        plt.ylabel(para_name[j])\n",
    "        plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of times to iterate over whole training dataset    \n",
    "iter_over_files =  1000\n",
    "# train_para and vali_para are storing f1score,auc,accuracy for \n",
    "# train and validation set.\n",
    "train_para = []\n",
    "vali_para = []\n",
    "\n",
    "now_time = time.time()\n",
    "for itera in range(0, iter_over_files):\n",
    "    print \"iteration %d\"%itera\n",
    "        \n",
    "    train_data = [train_batch[:5000], seq_batch[:5000], target_batch[:5000]]\n",
    "    train_(train_data)\n",
    "    \n",
    "    if 50 == itera:\n",
    "        validation_data = [train_batch[-5000:], seq_batch[-5000:], target_batch[-5000:]]\n",
    "        evaluate_(validation_data, 0)\n",
    "        evaluate_(train_data, 1)\n",
    "        \n",
    "        time_taken = (time.time()-now_time)/60.0\n",
    "        print \"time(in minutes) for 50 iter %f\"%(time_taken)\n",
    "        print \"remaining time(in minutes) %f\"%((time_taken/50.0)*(iter_over_files-1-itera))\n",
    "        now_time = time.time()\n",
    "\n",
    "t_para = np.asarray(train_para)\n",
    "v_para = np.asarray(vali_para)\n",
    "\n",
    "plot_parameters(t_para, v_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict values for test data set\n",
    "print ('opening file for write')\n",
    "label = [\"IdFeedBack,Prediction\"]\n",
    "with open('output.csv', 'wb') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=' ')\n",
    "    spamwriter.writerow(label)\n",
    "\n",
    "    for file_ in test_file_dir:\n",
    "        # get features from current indexed file\n",
    "        features = get_test_data(file_)        \n",
    "        # training features is np array with max size and padding\n",
    "        test_features, seq_len = prepare_data(features, max_seq_len)\n",
    "        # scale features to have 0 mean and 1 variance\n",
    "        scaled_features = feature_scaling(test_features, batch_size,\\\n",
    "                seq_len)\n",
    "        # find independent components from given features\n",
    "        inde_features = independent_components(scaled_features, batch_size,\\\n",
    "                seq_len)\n",
    "        # evaluate over whole file\n",
    "        y = np.zeros(0)\n",
    "        if \"Sess05\" in file_:\n",
    "            indices = [\"%03d\" %(i+1) for i in range(100)]\n",
    "    #        print file_\n",
    "            for index in range(0,100,batch_size):\n",
    "                yfile = observe.eval(feed_dict = {\\\n",
    "                    x:inde_features[index:index+batch_size], \\\n",
    "                    s_len:seq_len[index:index+batch_size]})\n",
    "                y = np.append(y,yfile)\n",
    "        else:\n",
    "            indices = [\"%03d\" %(i+1) for i in range(60)]\n",
    "            for index in range(0,60,batch_size):\n",
    "                yfile = observe.eval(feed_dict = {\\\n",
    "                    x:inde_features[index:index+batch_size], \\\n",
    "                    s_len:seq_len[index:index+batch_size]})\n",
    "                y = np.append(y,yfile)\n",
    "\n",
    "        for i in xrange(0,y.shape[0]):\n",
    "            values = [file_[5:-4]+'_FB'+indices[i]+','+str(int(y[i]))]\n",
    "            spamwriter.writerow(values)\n",
    "print ('file write complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
